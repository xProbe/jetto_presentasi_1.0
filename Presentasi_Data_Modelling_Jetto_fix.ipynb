{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4e3202",
   "metadata": {},
   "source": [
    "# DYNAMIC PRICING ENGINE - JETTO.ID\n",
    "## Data Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3e290",
   "metadata": {},
   "source": [
    "### library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03861f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e5e432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DYNAMIC PRICING ENGINE - JETTO.ID\n",
      "Fase 2: Modeling & Hyperparameter Tuning\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DYNAMIC PRICING ENGINE - JETTO.ID\")\n",
    "print(\"Fase 2: Modeling & Hyperparameter Tuning\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2b919",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ac769d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. MEMUAT DATA (DATA LOADING)\n",
      "============================================================\n",
      "✓ Berhasil memuat Analytics Base Table: (497, 24)\n",
      "✓ Berhasil memuat Transaksi Jetto: (50000, 19)\n",
      "\n",
      "Kolom ABT:\n",
      "['product_id', 'market_price_mean', 'market_price_std', 'market_price_min', 'market_price_max', 'jetto_price_mean', 'jetto_price_std', 'jetto_price_min', 'jetto_price_max', 'total_quantity_sold', 'avg_quantity_sold', 'std_quantity_sold', 'total_revenue', 'avg_revenue', 'std_revenue', 'transaction_count', 'weekend_ratio', 'peak_hour_ratio', 'price_elasticity', 'category', 'product_name', 'price_discount_avg', 'revenue_per_transaction', 'price_volatility']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. MEMUAT DATA (DATA LOADING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "abt = pd.read_csv('04_analytics_base_table.csv')\n",
    "print(f\"✓ Berhasil memuat Analytics Base Table: {abt.shape}\")\n",
    "\n",
    "df_jetto = pd.read_csv('03_jetto_simulated_transactions.csv')\n",
    "print(f\"✓ Berhasil memuat Transaksi Jetto: {df_jetto.shape}\")\n",
    "\n",
    "print(f\"\\nKolom ABT:\")\n",
    "print(abt.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc03cad",
   "metadata": {},
   "source": [
    "### Feature engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9411597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. FEATURE ENGINEERING UNTUK MODELING\n",
      "============================================================\n",
      "✓ Shape ABT setelah diperkaya: (497, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"2. FEATURE ENGINEERING UNTUK MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target = 'jetto_price_mean'\n",
    "\n",
    "transaction_features = df_jetto.groupby('product_id').agg({\n",
    "    'season': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'normal',\n",
    "    'day_of_week': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Monday',\n",
    "    'profit_margin_pct': 'mean',\n",
    "    'stock_level': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "transaction_features.columns = ['product_id', 'dominant_season', 'dominant_day', \n",
    "                                 'avg_profit_margin', 'avg_stock_level']\n",
    "\n",
    "abt_full = abt.merge(transaction_features, on='product_id', how='left')\n",
    "\n",
    "print(f\"✓ Shape ABT setelah diperkaya: {abt_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f2d38",
   "metadata": {},
   "source": [
    "### feature selection & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e767ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. SELEKSI FITUR & PREPROCESSING\n",
      "============================================================\n",
      "\n",
      "✓ Shape Matriks Fitur: (497, 21)\n",
      "✓ Variabel Target: jetto_price_mean\n",
      "✓ Statistik Target:\n",
      "count    497.000000\n",
      "mean       3.171383\n",
      "std        2.419438\n",
      "min        0.418673\n",
      "25%        1.556415\n",
      "50%        2.332151\n",
      "75%        4.188242\n",
      "max       13.035222\n",
      "Name: jetto_price_mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"3. SELEKSI FITUR & PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numeric_features = [\n",
    "    'market_price_mean', 'market_price_std', 'market_price_min', 'market_price_max',\n",
    "    'total_quantity_sold', 'avg_quantity_sold', 'std_quantity_sold',\n",
    "    'total_revenue', 'avg_revenue', 'std_revenue',\n",
    "    'transaction_count', 'weekend_ratio', 'peak_hour_ratio',\n",
    "    'price_elasticity', 'revenue_per_transaction', 'price_volatility',\n",
    "    'avg_profit_margin', 'avg_stock_level'\n",
    "]\n",
    "\n",
    "categorical_features = ['category', 'dominant_season', 'dominant_day']\n",
    "\n",
    "abt_full[numeric_features] = abt_full[numeric_features].fillna(abt_full[numeric_features].median())\n",
    "abt_full[categorical_features] = abt_full[categorical_features].fillna('unknown')\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    abt_full[f'{col}_encoded'] = le.fit_transform(abt_full[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "encoded_features = [f'{col}_encoded' for col in categorical_features]\n",
    "\n",
    "feature_columns = numeric_features + encoded_features\n",
    "X = abt_full[feature_columns]\n",
    "y = abt_full[target]\n",
    "\n",
    "print(f\"\\n✓ Shape Matriks Fitur: {X.shape}\")\n",
    "print(f\"✓ Variabel Target: {target}\")\n",
    "print(f\"✓ Statistik Target:\")\n",
    "print(y.describe())\n",
    "\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"\\n⚠ Peringatan: Ditemukan {X.isnull().sum().sum()} missing values. Mengisi dengan median...\")\n",
    "    X = X.fillna(X.median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22886ffe",
   "metadata": {},
   "source": [
    "\n",
    "### train test-splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0812bed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. TRAIN-TEST SPLIT\n",
      "============================================================\n",
      "✓ Training Set: 347 sampel (69.8%)\n",
      "✓ Validation Set: 75 sampel (15.1%)\n",
      "✓ Test Set: 75 sampel (15.1%)\n",
      "\n",
      "✓ Scaling fitur selesai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"4. TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)\n",
    "\n",
    "print(f\"✓ Training Set: {X_train.shape[0]} sampel ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"✓ Validation Set: {X_val.shape[0]} sampel ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"✓ Test Set: {X_test.shape[0]} sampel ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n✓ Scaling fitur selesai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc815b81",
   "metadata": {},
   "source": [
    "\n",
    "### baseline models (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39d1a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. BASELINE MODELS\n",
      "============================================================\n",
      "\n",
      "5.1 LINEAR REGRESSION\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 1.0000 | Val R²: 1.0000\n",
      "  Train RMSE: 0.0107 | Val RMSE: 0.0158\n",
      "\n",
      "5.2 RIDGE REGRESSION\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 0.9998 | Val R²: 0.9998\n",
      "\n",
      "5.3 LASSO REGRESSION\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 0.9982 | Val R²: 0.9983\n",
      "\n",
      "5.4 ELASTIC NET\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 0.9982 | Val R²: 0.9983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"5. BASELINE MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    \n",
    "    train_mape = np.mean(np.abs((y_train - y_train_pred) / (y_train + 1e-10))) * 100\n",
    "    val_mape = np.mean(np.abs((y_val - y_val_pred) / (y_val + 1e-10))) * 100\n",
    "    \n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Train_R2': train_r2,\n",
    "        'Val_R2': val_r2,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Val_RMSE': val_rmse,\n",
    "        'Train_MAE': train_mae,\n",
    "        'Val_MAE': val_mae,\n",
    "        'Train_MAPE': train_mape,\n",
    "        'Val_MAPE': val_mape\n",
    "    }\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "model_results = []\n",
    "\n",
    "print(\"\\n5.1 LINEAR REGRESSION\")\n",
    "print(\"-\" * 60)\n",
    "lr_results, lr_model = evaluate_model(\n",
    "    LinearRegression(), X_train_scaled, y_train, X_val_scaled, y_val, \n",
    "    \"Linear Regression\"\n",
    ")\n",
    "model_results.append(lr_results)\n",
    "print(f\"✓ Train R²: {lr_results['Train_R2']:.4f} | Val R²: {lr_results['Val_R2']:.4f}\")\n",
    "print(f\"  Train RMSE: {lr_results['Train_RMSE']:.4f} | Val RMSE: {lr_results['Val_RMSE']:.4f}\")\n",
    "\n",
    "print(\"\\n5.2 RIDGE REGRESSION\")\n",
    "print(\"-\" * 60)\n",
    "ridge_results, ridge_model = evaluate_model(\n",
    "    Ridge(alpha=1.0, random_state=42), X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    \"Ridge Regression\"\n",
    ")\n",
    "model_results.append(ridge_results)\n",
    "print(f\"✓ Train R²: {ridge_results['Train_R2']:.4f} | Val R²: {ridge_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n5.3 LASSO REGRESSION\")\n",
    "print(\"-\" * 60)\n",
    "lasso_results, lasso_model = evaluate_model(\n",
    "    Lasso(alpha=0.1, random_state=42), X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    \"Lasso Regression\"\n",
    ")\n",
    "model_results.append(lasso_results)\n",
    "print(f\"✓ Train R²: {lasso_results['Train_R2']:.4f} | Val R²: {lasso_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n5.4 ELASTIC NET\")\n",
    "print(\"-\" * 60)\n",
    "elasticnet_results, elasticnet_model = evaluate_model(\n",
    "    ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42), \n",
    "    X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "    \"Elastic Net\"\n",
    ")\n",
    "model_results.append(elasticnet_results)\n",
    "print(f\"✓ Train R²: {elasticnet_results['Train_R2']:.4f} | Val R²: {elasticnet_results['Val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589416dc",
   "metadata": {},
   "source": [
    "### advanced models (rf, gradient boosting, xgboost, lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b1595d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. ADVANCED MODELS\n",
      "============================================================\n",
      "\n",
      "6.1 RANDOM FOREST\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 0.9989 | Val R²: 0.9988\n",
      "\n",
      "6.2 GRADIENT BOOSTING\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 1.0000 | Val R²: 0.9995\n",
      "\n",
      "6.3 XGBOOST\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 1.0000 | Val R²: 0.9992\n",
      "\n",
      "6.4 LIGHTGBM\n",
      "------------------------------------------------------------\n",
      "✓ Train R²: 0.9891 | Val R²: 0.9911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"6. ADVANCED MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n6.1 RANDOM FOREST\")\n",
    "print(\"-\" * 60)\n",
    "rf_results, rf_model = evaluate_model(\n",
    "    RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=5,\n",
    "                         min_samples_leaf=2, random_state=42, n_jobs=-1),\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    \"Random Forest\"\n",
    ")\n",
    "model_results.append(rf_results)\n",
    "print(f\"✓ Train R²: {rf_results['Train_R2']:.4f} | Val R²: {rf_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n6.2 GRADIENT BOOSTING\")\n",
    "print(\"-\" * 60)\n",
    "gb_results, gb_model = evaluate_model(\n",
    "    GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                             min_samples_split=5, min_samples_leaf=2, \n",
    "                             random_state=42),\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    \"Gradient Boosting\"\n",
    ")\n",
    "model_results.append(gb_results)\n",
    "print(f\"✓ Train R²: {gb_results['Train_R2']:.4f} | Val R²: {gb_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n6.3 XGBOOST\")\n",
    "print(\"-\" * 60)\n",
    "xgb_results, xgb_model = evaluate_model(\n",
    "    xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                     subsample=0.8, colsample_bytree=0.8,\n",
    "                     random_state=42, n_jobs=-1),\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    \"XGBoost\"\n",
    ")\n",
    "model_results.append(xgb_results)\n",
    "print(f\"✓ Train R²: {xgb_results['Train_R2']:.4f} | Val R²: {xgb_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n6.4 LIGHTGBM\")\n",
    "print(\"-\" * 60)\n",
    "lgb_results, lgb_model = evaluate_model(\n",
    "    lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                     num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "                     random_state=42, n_jobs=-1, verbose=-1),\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    \"LightGBM\"\n",
    ")\n",
    "model_results.append(lgb_results)\n",
    "print(f\"✓ Train R²: {lgb_results['Train_R2']:.4f} | Val R²: {lgb_results['Val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a401ec8",
   "metadata": {},
   "source": [
    "### perbandingan antar model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b411e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. PERBANDINGAN MODEL\n",
      "============================================================\n",
      "\n",
      "Perbandingan Performa Model (diurutkan berdasarkan Validation R²):\n",
      "========================================================================================================================\n",
      "            Model  Train_R2   Val_R2  Train_RMSE  Val_RMSE  Train_MAE  Val_MAE  Train_MAPE  Val_MAPE\n",
      "Linear Regression  0.999980 0.999961    0.010666  0.015759   0.005366 0.007416    0.286766  0.266166\n",
      " Ridge Regression  0.999847 0.999760    0.029718  0.038858   0.019787 0.026350    0.830092  1.092186\n",
      "Gradient Boosting  1.000000 0.999483    0.001435  0.057041   0.000905 0.031409    0.045508  0.875438\n",
      "          XGBoost  0.999996 0.999175    0.004933  0.072059   0.003143 0.046151    0.144849  1.491916\n",
      "    Random Forest  0.998901 0.998818    0.079518  0.086239   0.021926 0.040330    0.529437  1.010888\n",
      "      Elastic Net  0.998240 0.998284    0.100648  0.103904   0.069467 0.076827    2.848084  3.096312\n",
      " Lasso Regression  0.998198 0.998269    0.101821  0.104343   0.077394 0.080488    3.942442  4.152610\n",
      "         LightGBM  0.989104 0.991085    0.250401  0.236814   0.071756 0.110393    1.672435  3.026353\n",
      "\n",
      "============================================================\n",
      " MODEL TERBAIK SEJAUH INI: Linear Regression\n",
      "   Validation R²: 1.0000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"7. PERBANDINGAN MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('Val_R2', ascending=False)\n",
    "\n",
    "print(\"\\nPerbandingan Performa Model (diurutkan berdasarkan Validation R²):\")\n",
    "print(\"=\"*120)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_val_r2 = results_df.iloc[0]['Val_R2']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\" MODEL TERBAIK SEJAUH INI: {best_model_name}\")\n",
    "print(f\"   Validation R²: {best_val_r2:.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a70a0",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711b992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "8. HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "8.1 TUNING RANDOM FOREST\n",
      "------------------------------------------------------------\n",
      "Menjalankan GridSearchCV untuk Random Forest...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "✓ Parameter Terbaik: {'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "✓ Best CV R²: 0.9965\n",
      "✓ Tuned Val R²: 0.9991\n",
      "\n",
      "8.2 TUNING XGBOOST\n",
      "------------------------------------------------------------\n",
      "Menjalankan RandomizedSearchCV untuk XGBoost...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "✓ Parameter Terbaik: {'subsample': 0.9, 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.9}\n",
      "✓ Best CV R²: 0.9978\n",
      "✓ Tuned Val R²: 0.9995\n",
      "\n",
      "8.3 TUNING LIGHTGBM\n",
      "------------------------------------------------------------\n",
      "Menjalankan RandomizedSearchCV untuk LightGBM...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 90\u001b[0m\n\u001b[0;32m     78\u001b[0m lgb_random \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     79\u001b[0m     lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     80\u001b[0m     lgb_param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMenjalankan RandomizedSearchCV untuk LightGBM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m \u001b[43mlgb_random\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Parameter Terbaik: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgb_random\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Best CV R²: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlgb_random\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1951\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"8. HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n8.1 TUNING RANDOM FOREST\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Menjalankan GridSearchCV untuk Random Forest...\")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✓ Parameter Terbaik: {rf_grid.best_params_}\")\n",
    "print(f\"✓ Best CV R²: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "rf_tuned = rf_grid.best_estimator_\n",
    "rf_tuned_results, _ = evaluate_model(rf_tuned, X_train, y_train, X_val, y_val, \"Random Forest (Tuned)\")\n",
    "print(f\"✓ Tuned Val R²: {rf_tuned_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n8.2 TUNING XGBOOST\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    xgb_param_grid,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Menjalankan RandomizedSearchCV untuk XGBoost...\")\n",
    "xgb_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✓ Parameter Terbaik: {xgb_random.best_params_}\")\n",
    "print(f\"✓ Best CV R²: {xgb_random.best_score_:.4f}\")\n",
    "\n",
    "xgb_tuned = xgb_random.best_estimator_\n",
    "xgb_tuned_results, _ = evaluate_model(xgb_tuned, X_train, y_train, X_val, y_val, \"XGBoost (Tuned)\")\n",
    "print(f\"✓ Tuned Val R²: {xgb_tuned_results['Val_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n8.3 TUNING LIGHTGBM\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'num_leaves': [31, 50],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    lgb_param_grid,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Menjalankan RandomizedSearchCV untuk LightGBM...\")\n",
    "lgb_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"✓ Parameter Terbaik: {lgb_random.best_params_}\")\n",
    "print(f\"✓ Best CV R²: {lgb_random.best_score_:.4f}\")\n",
    "\n",
    "lgb_tuned = lgb_random.best_estimator_\n",
    "lgb_tuned_results, _ = evaluate_model(lgb_tuned, X_train, y_train, X_val, y_val, \"LightGBM (Tuned)\")\n",
    "print(f\"✓ Tuned Val R²: {lgb_tuned_results['Val_R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c483c8",
   "metadata": {},
   "source": [
    "### pemilihan model (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"9. PEMILIHAN MODEL AKHIR (FINAL MODEL SELECTION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tuned_results = [rf_tuned_results, xgb_tuned_results, lgb_tuned_results]\n",
    "tuned_df = pd.DataFrame(tuned_results).sort_values('Val_R2', ascending=False)\n",
    "\n",
    "print(\"\\nPerforma Model Hasil Tuning:\")\n",
    "print(tuned_df.to_string(index=False))\n",
    "\n",
    "final_model_name = tuned_df.iloc[0]['Model']\n",
    "if 'Random Forest' in final_model_name:\n",
    "    final_model = rf_tuned\n",
    "elif 'XGBoost' in final_model_name:\n",
    "    final_model = xgb_tuned\n",
    "else:\n",
    "    final_model = lgb_tuned\n",
    "\n",
    "print(f\"\\n MODEL AKHIR TERPILIH: {final_model_name}\")\n",
    "print(f\"   Validation R²: {tuned_df.iloc[0]['Val_R2']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUASI MODEL AKHIR PADA TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mape = np.mean(np.abs((y_test - y_test_pred) / (y_test + 1e-10))) * 100\n",
    "\n",
    "print(f\"\\n✓ Test R²: {test_r2:.4f}\")\n",
    "print(f\"✓ Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"✓ Test MAE: {test_mae:.4f}\")\n",
    "print(f\"✓ Test MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "if test_r2 >= 0.85:\n",
    "    print(f\"\\n SUKSES: Model memenuhi target R² ≥ 0.85!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Model R² ({test_r2:.4f}) di bawah target 0.85, namun masih bisa diterima untuk prototipe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7dbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Pastikan Anda menggunakan variabel hasil prediksi yang sudah ada di notebook\n",
    "# Asumsi variabel: y_full (Nilai Aktual) dan y_pred (Nilai Prediksi Model)\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# 1. Scatter Plot: Actual vs Predicted Prices\n",
    "# Plot ini menggantikan Confusion Matrix untuk Regresi\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_full, y_pred, alpha=0.5, color='royalblue', s=10)\n",
    "# Garis diagonal sempurna (Target Ideal)\n",
    "min_val = min(y_full.min(), y_pred.min())\n",
    "max_val = max(y_full.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "plt.title('Actual vs Predicted Price', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Harga Aktual (Data)')\n",
    "plt.ylabel('Harga Prediksi (Model)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residual Plot (Analisis Error)\n",
    "# Mengecek apakah error tersebar acak (Model bagus) atau berpola (Model bias)\n",
    "residuals = y_full - y_pred\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(x=y_pred, y=residuals, alpha=0.5, color='orange', s=10)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.title('Residual Plot (Error Analysis)', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Harga Prediksi')\n",
    "plt.ylabel('Residual (Aktual - Prediksi)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribution of Errors (Histogram)\n",
    "# Memastikan error berdistribusi normal (Bell curve)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(residuals, bins=50, kde=True, color='green')\n",
    "plt.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "plt.title('Distribusi Error Prediksi', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Besar Error (£)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature Importance Plot (Bonus tapi Penting untuk Syarat No. 11)\n",
    "# Jika menggunakan model Tree-based (XGBoost/RandomForest)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Mengambil data feature importance (pastikan feature_columns tersedia)\n",
    "    feat_importances = pd.Series(final_model.feature_importances_, index=feature_columns)\n",
    "    feat_importances.nlargest(15).sort_values(ascending=True).plot(kind='barh', color='purple')\n",
    "    plt.title('Top 15 Fitur Paling Berpengaruh terhadap Penentuan Harga', fontsize=14)\n",
    "    plt.xlabel('Tingkat Kepentingan (Importance Score)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54183aa",
   "metadata": {},
   "source": [
    "### analisis fitur penting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"10. ANALISIS PENTINGNYA FITUR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Fitur Paling Penting:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    feature_importance.to_csv('05_feature_importance.csv', index=False)\n",
    "    print(\"\\n✓ Tereskpor: 05_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01541fc8",
   "metadata": {},
   "source": [
    "### penyimpanan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"11. PENYIMPANAN MODEL (MODEL PERSISTENCE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open('06_final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(\"✓ Tersimpan: 06_final_model.pkl\")\n",
    "\n",
    "with open('07_feature_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\" Tersimpan: 07_feature_scaler.pkl\")\n",
    "\n",
    "with open('08_label_encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(\" Tersimpan: 08_label_encoders.pkl\")\n",
    "\n",
    "with open('09_feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_columns, f)\n",
    "print(\" Tersimpan: 09_feature_columns.pkl\")\n",
    "\n",
    "model_metadata = {\n",
    "    'model_name': final_model_name,\n",
    "    'model_type': type(final_model).__name__,\n",
    "    'train_r2': tuned_df.iloc[0]['Train_R2'],\n",
    "    'val_r2': tuned_df.iloc[0]['Val_R2'],\n",
    "    'test_r2': test_r2,\n",
    "    'test_rmse': test_rmse,\n",
    "    'test_mae': test_mae,\n",
    "    'test_mape': test_mape,\n",
    "    'n_features': len(feature_columns),\n",
    "    'training_samples': len(X_train),\n",
    "    'target_variable': target\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame([model_metadata])\n",
    "metadata_df.to_csv('10_model_metadata.csv', index=False)\n",
    "print(\"✓ Tersimpan: 10_model_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d811a48d",
   "metadata": {},
   "source": [
    "### simpulan dan hasil akhir / ringkasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b099a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RINGKASAN FASE MODELING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Total Sampel: {len(X):,}\")\n",
    "print(f\"  - Training: {len(X_train):,}\")\n",
    "print(f\"  - Validation: {len(X_val):,}\")\n",
    "print(f\"  - Test: {len(X_test):,}\")\n",
    "print(f\"  - Jumlah Fitur: {len(feature_columns)}\")\n",
    "\n",
    "print(f\"\\nModel Final: {final_model_name}\")\n",
    "print(f\"  - Training R²: {tuned_df.iloc[0]['Train_R2']:.4f}\")\n",
    "print(f\"  - Validation R²: {tuned_df.iloc[0]['Val_R2']:.4f}\")\n",
    "print(f\"  - Test R²: {test_r2:.4f}\")\n",
    "print(f\"  - Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  - Test MAPE: {test_mape:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ FASE MODELING SELESAI DENGAN SUKSES!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSiap untuk Fase 3: Evaluasi & Deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
